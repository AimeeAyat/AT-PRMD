# AT-PRMD: Robust Multi-Objective Alignment via Pessimistic Reward Model Distillation

Implementation of "Robust Multi-Objective Alignment via Pessimistic Reward Model Distillation" using TRL and PyTorch 2.7.

## ğŸ¯ Project Overview

This project implements a pessimistic ensemble approach to reward model distillation for language model alignment. The goal is to train models that balance multiple competing objectives (helpfulness, harmlessness, honesty) without falling into degenerate solutions.

### Key Components
- **3 Reward Models**: One per objective (helpful, harmless, honest)
- **Base Model**: Qwen2.5-3B-Instruct
- **Dataset**: Anthropic HH-RLHF (~170k preference pairs)
- **Method**: Pessimistic DPO with ensemble reward models

## ğŸ”§ System Requirements

- **GPU**: RTX 5090 (32GB VRAM) or similar
- **CUDA**: 12.8
- **PyTorch**: 2.7.0
- **OS**: Windows (current setup)
- **Python**: 3.10+

## ğŸ“¦ Installation

### Step 1: Install PyTorch 2.7 with CUDA 12.8

```bash
# Install PyTorch with CUDA 12.8 support
pip install torch==2.7.0 torchvision==0.20.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Verify Setup

```bash
python utils.py
```

This will check your CUDA setup and create necessary directories.

## ğŸ“Š Project Structure

```
ATML_PROJ_IMP/
â”œâ”€â”€ 1_data_preparation/          # Data download and preprocessing
â”‚   â”œâ”€â”€ download_dataset.py      # Download HH-RLHF dataset
â”‚   â””â”€â”€ split_objectives.py      # Split by objectives
â”œâ”€â”€ 2_reward_modeling/           # Reward model training
â”‚   â”œâ”€â”€ train_helpful_rm.py      # Train helpful reward model
â”‚   â”œâ”€â”€ train_harmless_rm.py     # Train harmless reward model
â”‚   â””â”€â”€ train_honest_rm.py       # Train honest reward model
â”œâ”€â”€ 3_policy_training/           # Policy training with pessimistic DPO
â”‚   â”œâ”€â”€ baseline_dpo.py          # Baseline DPO
â”‚   â”œâ”€â”€ pessimistic_dpo.py       # Pessimistic DPO
â”‚   â””â”€â”€ hierarchical_dpo.py      # Hierarchical pessimistic DPO
â”œâ”€â”€ 4_evaluation/                # Evaluation scripts
â”‚   â”œâ”€â”€ compute_metrics.py       # Compute alignment metrics
â”‚   â””â”€â”€ benchmark_eval.py        # Run benchmarks
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â”œâ”€â”€ reward_model_config.yaml
â”‚   â””â”€â”€ policy_config.yaml
â”œâ”€â”€ data/                        # Data storage
â”‚   â”œâ”€â”€ raw/                     # Raw downloaded data
â”‚   â”œâ”€â”€ processed/               # Processed objective-specific data
â”‚   â””â”€â”€ cache/                   # HuggingFace cache
â”œâ”€â”€ models/                      # Model storage
â”‚   â”œâ”€â”€ reward_models/           # Trained reward models
â”‚   â””â”€â”€ policy_models/           # Trained policy models
â”œâ”€â”€ logs/                        # Training logs
â””â”€â”€ outputs/                     # Evaluation outputs
```

## ğŸš€ Usage

### Phase 1: Data Preparation

```bash
# Download the dataset
python 1_data_preparation/download_dataset.py

# Split by objectives
python 1_data_preparation/split_objectives.py
```

### Phase 2: Train Reward Models

```bash
# Train helpful reward model
python 2_reward_modeling/train_helpful_rm.py

# Train harmless reward model
python 2_reward_modeling/train_harmless_rm.py

# Train honest reward model
python 2_reward_modeling/train_honest_rm.py
```

### Phase 3: Train Policy Models

```bash
# Baseline DPO (for comparison)
python 3_policy_training/baseline_dpo.py

# Pessimistic DPO (main method)
python 3_policy_training/pessimistic_dpo.py
```

### Phase 4: Evaluation

```bash
# Compute metrics
python 4_evaluation/compute_metrics.py

# Run benchmarks
python 4_evaluation/benchmark_eval.py
```

## ğŸ”¬ Experiment Configurations

### Reward Model Training
- **Learning Rate**: 1e-5
- **Batch Size**: 4 (per device) Ã— 8 (gradient accumulation) = 32 effective
- **Epochs**: 3
- **Max Length**: 512 tokens
- **Precision**: BF16

### Policy Training
- **Learning Rate**: 5e-7
- **Batch Size**: 2 (per device) Ã— 16 (gradient accumulation) = 32 effective
- **Beta (KL penalty)**: 0.1
- **Max Length**: 512 tokens
- **Precision**: BF16

### Pessimism Methods
1. **Hard Minimum**: Take worst-case reward across ensemble
2. **CVaR-10%**: Average worst 10% of rewards
3. **Hierarchical**: Worst within objectives, then worst across

## ğŸ“ˆ Evaluation Metrics

- **Win Rate Against Reference**: Preference-based evaluation
- **Per-Objective Performance**: Separate scores for each objective
- **Worst-Case Performance**: Minimum across all objectives
- **KL Divergence**: Distance from reference policy
- **Benchmarks**:
  - HH-RLHF Holdout
  - MT-Bench
  - TruthfulQA
  - RealToxicityPrompts
  - JailbreakBench

## ğŸ”§ Configuration

Edit `configs/reward_model_config.yaml` and `configs/policy_config.yaml` to adjust hyperparameters.

## ğŸ“ Logging

- **TensorBoard**: `tensorboard --logdir logs/`
- **Weights & Biases**: Configure in YAML files

## âš ï¸ Notes

- Each reward model training takes ~2-4 hours on RTX 5090
- Policy training takes ~3-5 hours
- Total dataset size: ~10GB
- Model checkpoints: ~6GB per reward model

## ğŸ“š References

- Paper: "Robust Multi-Objective Alignment via Pessimistic Reward Model Distillation"
- Dataset: [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- Library: [TRL](https://huggingface.co/docs/trl/)
- Model: [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)

## ğŸ¤ Contributing

This is a research implementation. Feel free to modify and experiment!

## ğŸ“Š Dual-Approach Support

This implementation supports **two independent approaches** for comprehensive comparison:

### Approach A (Primary): HH-RLHF + TruthfulQA
- Helpful & Harmless from Anthropic HH-RLHF (clean, human-annotated)
- Honest from TruthfulQA (converted to preference pairs)
- 3 reward models with diverse data sources

### Approach B (Comparison): PKU-SafeRLHF
- All 3 objectives from PKU-SafeRLHF with native annotations
- Single data source, uniform distribution
- 3 reward models from same dataset

**Switch approaches** by editing `configs/reward_model_config.yaml`:
```yaml
data:
  approach: "hh_truthfulqa"  # or "pku_safe"
```

See [DUAL_APPROACH_GUIDE.md](DUAL_APPROACH_GUIDE.md) for detailed comparison.

## ğŸ“ˆ Visualization & Logging

Every step automatically generates:

### JSON Results
- Step results saved to `./outputs/step_results/`
- Timestamped for tracking experiments
- Includes all metrics and statistics

### Visualizations
1. **Dataset Statistics**: Train/val/test sizes per objective
2. **Text Length Distributions**: Character counts across objectives
3. **Sample Examples**: 10 random examples per objective (JSON + readable text)
4. **Reward Distributions**: Chosen vs rejected scores
5. **Reward Margins**: Score differences analysis
6. **Top/Bottom Examples**: Best and worst performing samples

Saved to: `./outputs/visualizations/` and `./outputs/reward_analysis/`

### TensorBoard Logging
```bash
# View training progress
tensorboard --logdir logs/
```

Tracks:
- Loss curves
- Accuracy
- Learning rate
- Gradient norms
- Custom metrics

### Checkpointing Strategy
- **5 checkpoints saved**: Start, 33%, 66%, End, Best
- **Best model** automatically loaded at end
- Checkpoints in: `./models/reward_models/<objective>_rm/`

## ğŸ“„ License

Research and educational use.
